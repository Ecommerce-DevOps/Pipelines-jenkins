pipeline {
    agent any
    
    parameters {
        string(name: 'IMAGE_TAG', defaultValue: 'latest-dev', description: 'Tag de la imagen a desplegar (e.g., latest-dev, commit-sha)')
        string(name: 'NOTIFICATION_EMAIL', defaultValue: 'geoffreypv00@gmail.com', description: 'Email para notificaciones de pipeline')
    }

    environment {
        IMAGE_NAME = "order-service"
        GCR_REGISTRY = "us-central1-docker.pkg.dev/rock-fortress-479417-t5/ecommerce-microservices"
        FULL_IMAGE_NAME = "${GCR_REGISTRY}/${IMAGE_NAME}"
        
        IMAGE_TAG = "${params.IMAGE_TAG}" 
        
        GCP_CREDENTIALS = credentials('gke-credentials')
        GCP_PROJECT = "rock-fortress-479417-t5"
        
        CLUSTER_NAME = "ecommerce-devops-cluster" 
        CLUSTER_LOCATION_FLAG = "--region=us-central1"
        
        K8S_NAMESPACE = "staging"
        K8S_DEPLOYMENT_NAME = "order-service"
        K8S_CONTAINER_NAME = "order-service"
        K8S_SERVICE_NAME = "order-service"
        SERVICE_PORT = "8300" 
        
        API_GATEWAY_SERVICE_NAME = "api-gateway"
    }

    stages {
        stage('Install Parent POM') {
            steps {
                cleanWs()
                dir('parent-repo') {
                    git branch: 'main', 
                        url: 'https://github.com/Ecommerce-DevOps/General-config.git', 
                        credentialsId: 'github-credentials'
                    
                    script {
                        docker.image('maven:3.8.4-openjdk-11').inside('-v maven-repo:/root/.m2') {
                            sh 'mvn clean install -N' 
                        }
                    }
                }
            }
        }
        
        stage('Checkout SCM') {
            steps {
                cleanWs()
                dir('Scripts') {
                    git branch: 'main', url: 'https://github.com/Ecommerce-DevOps/Scripts.git', credentialsId: 'github-credentials'
                }
                dir('manifests-gcp') {
                    git branch: 'main', url: 'https://github.com/Ecommerce-DevOps/Manifests-kubernetes-helms.git', credentialsId: 'github-credentials'
                }
                dir('tests') {
                    git branch: 'main', url: 'https://github.com/Ecommerce-DevOps/Testing-unit-integration-e2e-locust.git', credentialsId: 'github-credentials'
                }
                dir('order-service') {
                    git branch: 'main', url: 'https://github.com/Ecommerce-DevOps/order-service.git', credentialsId: 'github-credentials'
                }

                echo "üì¶ Iniciando despliegue a STAGING"
                echo "üì¶ Imagen a desplegar: ${FULL_IMAGE_NAME}:${IMAGE_TAG}"
            }
        }

        stage('Generate Release Notes') {
            steps {
                script {
                    dir('order-service') {
                        sh """
                            echo "üìù Generando Release Notes..."
                            cp ../Scripts/Infra/generate-release-notes.sh .
                            chmod +x generate-release-notes.sh
                            ./generate-release-notes.sh release-notes.txt
                        """
                        archiveArtifacts artifacts: 'release-notes.txt', allowEmptyArchive: true
                    }
                }
            }
        }

        stage('Authenticate GCP & Kubernetes') {
            steps {
                script {
                    sh """
                        echo "üîê Autenticando con GCP..."
                        gcloud auth activate-service-account --key-file=${GCP_CREDENTIALS}
                        gcloud config set project ${GCP_PROJECT}
                        gcloud auth configure-docker us-central1-docker.pkg.dev --quiet
                        echo "‚ò∏Ô∏è Obteniendo credenciales de GKE..."
                        gcloud container clusters get-credentials ${CLUSTER_NAME} ${CLUSTER_LOCATION_FLAG} --project ${GCP_PROJECT}
                    """
                }
            }
        }

        stage('Verify Image Exists in GCR') {
            steps {
                script {
                    sh """
                        echo "üîç Verificando ${FULL_IMAGE_NAME}:${IMAGE_TAG}..."
                        gcloud artifacts docker images describe ${FULL_IMAGE_NAME}:${IMAGE_TAG} || {
                            echo "‚ùå ERROR: Imagen no encontrada"
                            echo "Aseg√∫rate de que el pipeline de DEV ('order-service-pipeline.groovy') haya corrido exitosamente."
                            exit 1
                        }
                        echo "‚úÖ Imagen verificada."
                    """
                }
            }
        }
        
        stage('Deploy to Staging (Helm)') {
            steps {
                script {
                    sh """
                        echo "üöÄ Desplegando a ${K8S_NAMESPACE} usando Helm..."
                        kubectl create namespace ${K8S_NAMESPACE} --dry-run=client -o yaml | kubectl apply -f -
                        
                        echo "üìã Aplicando/Actualizando Chart de Helm: ${K8S_DEPLOYMENT_NAME}"
                        
                        helm upgrade --install ${K8S_DEPLOYMENT_NAME} manifests-gcp/${K8S_DEPLOYMENT_NAME}/ \
                            --namespace ${K8S_NAMESPACE} \
                            --set image.repository=${FULL_IMAGE_NAME} \
                            --set image.tag=${IMAGE_TAG} \
                            --set service.port=${SERVICE_PORT} \
                            --wait --timeout=5m
                        
                        echo "‚úÖ Despliegue completado."
                    """
                }
            }
        }

        stage('Health Check & Smoke Tests') {
            steps {
                script {
                    sh """
                        echo "üè• Ejecutando health checks..."
                        
                        kubectl wait --for=condition=ready pod \
                            -l app=${K8S_DEPLOYMENT_NAME} \
                            -n ${K8S_NAMESPACE} \
                            --timeout=300s
                        
                        echo "üéØ Verificando endpoint de salud internamente..."
                        
                        # USAR UN POD EXTERNO CON CURL EN LUGAR DE ENTRAR AL POD DE LA APP
                        kubectl run health-check-${BUILD_NUMBER} \
                            --image=curlimages/curl:latest \
                            -n ${K8S_NAMESPACE} \
                            --rm -i --restart=Never \
                            -- \
                            curl -f -v http://${K8S_SERVICE_NAME}:${SERVICE_PORT}/${K8S_SERVICE_NAME}/actuator/health || {
                                echo "‚ö†Ô∏è Health check fall√≥"
                                kubectl logs -l app=${K8S_DEPLOYMENT_NAME} -n ${K8S_NAMESPACE} --tail=50
                                exit 1
                            }
                        
                        echo "‚úÖ Health check passed!"
                    """
                }
            }
        }

        stage('Verify Gateway Availability') {
            steps {
                script {
                    sh """
                        echo "üåê Verificando disponibilidad del API Gateway (${API_GATEWAY_SERVICE_NAME})..."
                        
                        kubectl wait --for=condition=ready pod \
                            -l app.kubernetes.io/name=${API_GATEWAY_SERVICE_NAME} \
                            -n ${K8S_NAMESPACE} \
                            --timeout=300s
                        
                        GATEWAY_IP=\$(kubectl get svc ${API_GATEWAY_SERVICE_NAME} -n ${K8S_NAMESPACE} \
                            -o jsonpath='{.spec.clusterIP}')
                        
                        if [ -z "\$GATEWAY_IP" ]; then
                            echo "‚ùå No se pudo obtener la IP del servicio ${API_GATEWAY_SERVICE_NAME}"
                            exit 1
                        fi
                        
                        echo "‚úÖ Gateway ClusterIP: \$GATEWAY_IP"
                        echo "\$GATEWAY_IP" > gateway-ip.txt
                        
                        echo "üîç Verificando conectividad al Gateway en http://\$GATEWAY_IP:8080/actuator/health"
                        kubectl run test-gateway-${BUILD_NUMBER} --image=curlimages/curl:latest \
                            -n ${K8S_NAMESPACE} --rm -i --restart=Never --timeout=60s -- \
                            curl -f --retry 5 --retry-delay 5 --retry-connrefused \
                            http://\$GATEWAY_IP:8080/actuator/health || {
                                echo "‚ö†Ô∏è No se pudo conectar al Gateway internamente"
                                exit 1
                            }
                        
                        echo "‚úÖ Gateway respondiendo correctamente"
                    """
                }
            }
        }

        stage('Verify Service Registration') {
            steps {
                script {
                    sh """
                        echo "üîç Verificando registro en Eureka..."
                        
                        # Retry loop for Eureka registration
                        for i in {1..30}; do
                            if kubectl run eureka-check-${BUILD_NUMBER} --image=curlimages/curl:latest \
                                -n ${K8S_NAMESPACE} --rm -i --restart=Never -- \
                                curl -s -f http://discovery:8761/eureka/apps/ORDER-SERVICE | grep -q "UP"; then
                                echo "‚úÖ ORDER-SERVICE registrado y UP en Eureka"
                                break
                            fi
                            
                            echo "‚è≥ Esperando a que ORDER-SERVICE se registre en Eureka... (\$i/30)"
                            kubectl delete pod eureka-check-${BUILD_NUMBER} -n ${K8S_NAMESPACE} --force --grace-period=0 2>/dev/null || true
                            sleep 5
                            
                            if [ \$i -eq 30 ]; then
                                echo "‚ùå Timeout esperando registro en Eureka"
                                exit 1
                            fi
                        done
                    """
                }
            }
        }

        stage('Run E2E Tests (Maven)') {
            when {
                expression { fileExists('tests/e2e/pom.xml') }
            }
            steps {
                script {
                    // Inicializamos variable para capturar el estado
                    def testsFailed = false
                    
                    try {
                        sh """
                            echo "üåê =============================================="
                            echo "üåê Configurando URL del API Gateway"
                            echo "üåê =============================================="
                            
                            GATEWAY_URL="http://api-gateway.${K8S_NAMESPACE}:8080"
                            echo "Gateway URL: \$GATEWAY_URL"
                            
                            # Verificar conectividad (Este s√≠ lo dejamos fallar si no hay red, es cr√≠tico)
                            kubectl run test-gateway-conn --image=curlimages/curl:latest --rm -i --restart=Never -n ${K8S_NAMESPACE} -- \
                                curl -s -o /dev/null -w "%{http_code}" \$GATEWAY_URL/actuator/health || {
                                    echo "‚ùå api-gateway no responde. Abortando tests."
                                    exit 1
                                }
                            
                            echo "‚úÖ api-gateway respondiendo correctamente"
                            
                            # Crear Pod de Tests
                            cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: e2e-test-runner-${BUILD_NUMBER}
  namespace: ${K8S_NAMESPACE}
spec:
  restartPolicy: Never
  containers:
  - name: maven-tests
    image: maven:3.9.9-eclipse-temurin-17
    command: ["sleep"]
    args: ["3600"]
    workingDir: /workspace
EOF

                            # Esperar Pod
                            echo "‚è≥ Esperando a que el pod de tests est√© listo..."
                            kubectl wait --for=condition=ready pod/e2e-test-runner-${BUILD_NUMBER} -n ${K8S_NAMESPACE} --timeout=120s
                            
                            # Copiar C√≥digo
                            echo "üì¶ Copiando c√≥digo de tests al pod..."
                            kubectl cp tests/e2e e2e-test-runner-${BUILD_NUMBER}:/workspace/e2e -n ${K8S_NAMESPACE}
                            
                            # Ejecutar Tests (Notar el cambio aqu√≠: capturamos el exit code)
                            echo "üß™ Ejecutando tests E2E..."
                            if ! kubectl exec -n ${K8S_NAMESPACE} e2e-test-runner-${BUILD_NUMBER} -- \
                                mvn clean test -f /workspace/e2e/pom.xml \
                                -Dapi.gateway.url=\$GATEWAY_URL \
                                -Dmaven.test.failure.ignore=true; then
                                echo "‚ö†Ô∏è Tests devolvieron error, pero continuaremos..."
                                # Creamos un archivo bandera para saber que fall√≥ fuera del shell
                                touch tests_failed_flag
                            fi
                            
                            # Copiar Resultados SIEMPRE
                            echo "üìã Copiando resultados de tests..."
                            kubectl cp e2e-test-runner-${BUILD_NUMBER}:/workspace/e2e/target tests/e2e/ -n ${K8S_NAMESPACE} || true
                            
                            # Limpiar
                            kubectl delete pod e2e-test-runner-${BUILD_NUMBER} -n ${K8S_NAMESPACE} || true
                        """
                        
                        // Verificamos si se cre√≥ el archivo bandera de fallo
                        if (fileExists('tests_failed_flag')) {
                            testsFailed = true
                            sh "rm tests_failed_flag" // Limpiar
                        }

                    } catch (Exception e) {
                        echo "‚ö†Ô∏è Error ejecutando la etapa de tests: ${e.message}"
                        testsFailed = true
                    }

                    // L√≥gica para NO fallar la pipeline
                    if (testsFailed) {
                        echo "‚ö†Ô∏è Los tests E2E fallaron, marcando build como UNSTABLE pero continuando..."
                        currentBuild.result = 'UNSTABLE'
                    } else {
                        echo "‚úÖ E2E Tests completados exitosamente."
                    }
                }
            }
            post {
                always {
                    // JUnit reportar√° los fallos visualmente sin detener la pipeline gracias a allowEmptyResults
                    junit allowEmptyResults: true, testResults: 'tests/e2e/target/surefire-reports/*.xml'
                    archiveArtifacts artifacts: 'tests/e2e/target/surefire-reports/**/*', allowEmptyArchive: true
                }
            }
        }

        stage('Security Scan (OWASP ZAP)') {
            steps {
                script {
                    sh """
                        echo "üõ°Ô∏è =============================================="
                        echo "üõ°Ô∏è Ejecutando Escaneo de Seguridad OWASP ZAP (In-Cluster)"
                        echo "üõ°Ô∏è =============================================="
                        
                        # URL interna del api-gateway
                        TARGET_URL="http://api-gateway.\${K8S_NAMESPACE}:8080"
                        echo "Target URL: \$TARGET_URL"
                        
                        mkdir -p reports/zap
                        
                        # Ejecutar ZAP dentro del cluster usando un Pod temporal
                        echo "üõ°Ô∏è Desplegando pod de OWASP ZAP en el cluster..."
                        
                        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: zap-scanner-\${BUILD_NUMBER}
  namespace: \${K8S_NAMESPACE}
spec:
  restartPolicy: Never
  securityContext:
    runAsUser: 0
  containers:
  - name: zap
    image: ghcr.io/zaproxy/zaproxy:stable
    command: ["sleep"]
    args: ["3600"]
    workingDir: /zap/wrk
EOF

                        # Esperar a que el pod est√© listo
                        echo "‚è≥ Esperando a que el pod de ZAP est√© listo..."
                        kubectl wait --for=condition=ready pod/zap-scanner-\${BUILD_NUMBER} -n \${K8S_NAMESPACE} --timeout=120s
                        
                        # Ejecutar ZAP Baseline Scan dentro del pod
                        echo "üõ°Ô∏è Ejecutando escaneo ZAP..."
                        kubectl exec -n \${K8S_NAMESPACE} zap-scanner-\${BUILD_NUMBER} -- \
                            zap-baseline.py -t \$TARGET_URL -r zap_report.html -I || echo "‚ö†Ô∏è ZAP encontr√≥ alertas, revisar reporte."
                        
                        # Copiar reporte de vuelta
                        echo "üìã Copiando reporte de ZAP..."
                        kubectl cp zap-scanner-\${BUILD_NUMBER}:/zap/wrk/zap_report.html reports/zap/zap_report.html -n \${K8S_NAMESPACE} || true
                        
                        # Limpiar pod
                        echo "üßπ Limpiando pod de ZAP..."
                        kubectl delete pod zap-scanner-\${BUILD_NUMBER} -n \${K8S_NAMESPACE} || true
                            
                        echo "‚úÖ Escaneo de seguridad completado."
                    """
                }
            }
            post {
                always {
                    publishHTML([
                        allowMissing: true,
                        alwaysLinkToLastBuild: true,
                        keepAll: true,
                        reportDir: 'reports/zap',
                        reportFiles: 'zap_report.html',
                        reportName: 'OWASP ZAP Security Report',
                        reportTitles: 'ZAP Security Scan Results'
                    ])
                }
            }
        }

        stage('Run Performance Tests (Locust)') {
            when {
                expression { fileExists('tests/performance/simple_load_test.py') }
            }
            steps {
                script {
                    sh """
                        echo "üöÄ =============================================="
                        echo "üöÄ Ejecutando Performance Tests con Locust (In-Cluster)"
                        echo "üöÄ =============================================="
                        
                        # URL interna del api-gateway
                        TARGET_HOST="http://api-gateway.${K8S_NAMESPACE}:8080"
                        
                        echo "Target Host: \$TARGET_HOST"
                        
                        # Crear directorio para reportes
                        mkdir -p reports
                        
                        # Ejecutar Locust dentro del cluster usando un Pod temporal
                        # Montamos el script de test usando ConfigMap o copi√°ndolo (aqu√≠ usaremos copia)
                        
                        echo "üì¶ Preparando pod de Locust..."
                        
                        cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: locust-runner-\${BUILD_NUMBER}
  namespace: \${K8S_NAMESPACE}
spec:
  restartPolicy: Never
  securityContext:
    runAsUser: 0
  containers:
  - name: locust
    image: locustio/locust
    command: ["sleep"]
    args: ["3600"]
    workingDir: /home/locust
EOF

                        # Esperar a que el pod est√© listo
                        echo "‚è≥ Esperando a que el pod de Locust est√© listo..."
                        kubectl wait --for=condition=ready pod/locust-runner-\${BUILD_NUMBER} -n \${K8S_NAMESPACE} --timeout=120s
                        
                        # Crear directorio de trabajo y copiar scripts
                        echo "üì¶ Preparando directorio de trabajo..."
                        kubectl exec -n \${K8S_NAMESPACE} locust-runner-\${BUILD_NUMBER} -- mkdir -p /home/locust/performance
                        
                        # Copiar el script de test al pod
                        echo "üì¶ Copiando script de tests al pod..."
                        kubectl cp tests/performance/. locust-runner-\${BUILD_NUMBER}:/home/locust/performance/ -n \${K8S_NAMESPACE}
                        
                        # Ejecutar Locust dentro del pod
                        echo "üöÄ Ejecutando Locust..."
                        kubectl exec -n \${K8S_NAMESPACE} locust-runner-\${BUILD_NUMBER} -- \
                            locust -f /home/locust/performance/simple_load_test.py \
                            --host \$TARGET_HOST \
                            --users 50 --spawn-rate 5 --run-time 1m \
                            --headless \
                            --csv=/home/locust/locust_stats || LOCUST_FAILED=true
                            
                        # Copiar resultados de vuelta
                        echo "üìã Copiando reportes de Locust..."
                        kubectl cp locust-runner-\${BUILD_NUMBER}:/home/locust/locust_stats_stats.csv reports/locust_stats.csv -n \${K8S_NAMESPACE} || true
                        
                        # Limpiar pod
                        echo "üßπ Limpiando pod de Locust..."
                        kubectl delete pod locust-runner-\${BUILD_NUMBER} -n \${K8S_NAMESPACE} || true
                        
                        if [ "\$LOCUST_FAILED" = "true" ]; then
                            echo "‚ùå Performance tests fallaron"
                            exit 1
                        fi
                        
                        echo "‚úÖ Performance tests completados"
                        
                        # Mostrar estad√≠sticas si existen
                        if [ -f "reports/locust_stats.csv" ]; then
                            echo ""
                            echo "üìä =============================================="
                            echo "üìä RESUMEN DE PERFORMANCE TESTS"
                            echo "üìä =============================================="
                            cat reports/locust_stats.csv
                        fi
                    """
                }
            }
            post {
                always {
                    script {
                        sh "pkill -f 'kubectl port-forward.*proxy-client.*8100' || true"
                    }
                    archiveArtifacts artifacts: 'reports/**/*', allowEmptyArchive: true
                    publishHTML([
                        allowMissing: true,
                        alwaysLinkToLastBuild: true,
                        keepAll: true,
                        reportDir: 'reports',
                        reportFiles: 'load_test_report.html',
                        reportName: 'Locust Performance Report',
                        reportTitles: 'Performance Test Results'
                    ])
                }
            }
        }
    }

    post {
        success {
            script {
                sh """
                    echo "üéâ ‚úÖ STAGING DEPLOY EXITOSO"
                    echo "üì¶ Imagen desplegada: ${FULL_IMAGE_NAME}:${IMAGE_TAG}"
                    gcloud auth activate-service-account --key-file=${GCP_CREDENTIALS}
                    gcloud auth revoke --all || true
                """
                echo "üìß Enviando notificaci√≥n de √âXITO a ${params.NOTIFICATION_EMAIL}..."
                // mail to: "${params.NOTIFICATION_EMAIL}",
                //      subject: "Deploy Staging Exitoso: ${IMAGE_NAME}",
                //      body: "El despliegue a Staging de ${IMAGE_NAME}:${IMAGE_TAG} ha sido exitoso."
            }
        }
        failure {
            script {
                sh """
                    echo "üîê Re-autenticando para operaciones de rollback..."
                    gcloud auth activate-service-account --key-file=${GCP_CREDENTIALS}
                    gcloud config set project ${GCP_PROJECT}
                    gcloud container clusters get-credentials ${CLUSTER_NAME} ${CLUSTER_LOCATION_FLAG} --project ${GCP_PROJECT}
                """
                
                def failedStage = env.STAGE_NAME ?: 'Unknown'
                
                sh """
                    echo "‚ùå üí• STAGING DEPLOY FALL√ì"
                    echo "üîç Fallo detectado en stage: ${failedStage}"
                    
                    if [ "${failedStage}" = "Deploy to Staging (Helm)" ]; then
                        echo "üîÑ Realizando rollback del despliegue fallido..."
                        helm rollback ${K8S_DEPLOYMENT_NAME} 0 -n ${K8S_NAMESPACE} || echo "‚ö†Ô∏è No hay revisi√≥n anterior para rollback."
                    else
                        echo "‚ö†Ô∏è Fallo en stage '${failedStage}'. El despliegue NO ser√° revertido."
                    fi
                    
                    echo "üìã Informaci√≥n de debug:"
                    kubectl get events -n ${K8S_NAMESPACE} --sort-by='.lastTimestamp' | tail -20
                    gcloud auth revoke --all || true
                """
                echo "üìß Enviando notificaci√≥n de FALLO a ${params.NOTIFICATION_EMAIL}..."
                // mail to: "${params.NOTIFICATION_EMAIL}",
                //      subject: "Deploy Staging FALLIDO: ${IMAGE_NAME}",
                //      body: "El despliegue a Staging de ${IMAGE_NAME} ha fallado en el stage '${failedStage}'. Revisar logs en Jenkins."
            }
        }
        always {
            cleanWs()
        }
    }
}
